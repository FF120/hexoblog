---
title: 反向传播算法及简单实例
toc: true
categories:
  - 机器学习
tags:
  - BP
date: 2017-04-19 10:22:48
---
反向传播算法入门资源索引：
http://www.52nlp.cn/tag/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95python%E4%BB%A3%E7%A0%81
<!-- more -->



一个简单的三层网络：
![](2017-04-19_191339.png)
下面我们以上面的简单的三层的神经网络说明BP算法的计算过程。
初始的输入参数是：

|$x_1$|$x_2$|$x_3$|$w_{14}$|$w_{15}$|$w_{24}$|$w_25$|$w_{34}$|$w_{35}$|$w_{46}$|$w_{56}$|$b_4$|$b_5$|$b_6$|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|1|0|1|0.2|-0.3|0.4|0.1|-0.5|0.2|-0.3|-0.2|-0.4|0.2|0.1|

该神经网络接受一个三维的向量作为输入[x1,x2,x3],输出只有一个数字，这里设置为不是0，就是1.示例给出的数据[1,0,1]对应的类别是1.

定义好了网络的结构，给出了初始的网络参数的值( $w_{14}$|$w_{15}$|$w_{24}$|$w_25$|$w_{34}$|$w_{35}$|$w_{46}$|$w_{56}$|$b_4$|$b_5$|$b_6$| ),都是该神经网络的参数。现在的目标是使得该神经网络在输入是[1,0,1]时，输出是1,或者至少是十分接近1.

首先计算一下在初始化参数的情况下，神经网络的输出是什么。
神经元输出的计算过程。
![](2017-04-19_102750.png)

在这里，激活函数我们使用sigmiod函数：

$$
f(x) =  \dfrac 1 {1+e^{-x}}
$$

我们将第i个神经元在激活函数作用之前的输出记作的输出记作 $I_i$ ,在激活函数作用之后的值记 $O_i$
计算过程如下：
$$
I_4 = x_1*w_{14}+x_2*w_{24}+x_3*w_{34}+b_4 = 1*0.2+0*0.4+1*(-0.5)+(-0.4) = -0.7
$$

$$
I_5 = x_1*w_{15}+x_2*w_{25}+x_3*w_{35}+b_5 = 1*(-0.3)+0*0.1+1*0.2+0.2 = 0.1
$$

$$
I_6 = O_4*w_{46}+O_5*w_{56}+b_6 = 0.331812*(-0.3)+0.524979*(-0.2)+0.1 = -0.104539
$$


$$
O_4 = f(I_4) = \dfrac 1 {1+e^{-(-0.7)}} = 0.331812
$$

$$
O_5 = f(I_5) = \dfrac 1 {1+e^{-0.1}} = 0.524979
$$

$$
O_6 = f(I_6) = \dfrac 1 {1+e^{-(-0.104539)}} = 0.473889
$$

可以看到，初始化的参数对[1,0,1]的输出是0.473899，而我们期望的输出是1。 所以，必须使用某种办法调整参数，才可以达到我们的期望。

我们定义误差如下：
$$
E = \frac12 (target - O_6)^2
$$ 
用平方误差来度量输出和期望的输出之间的差距。那么接下来我们希望调整神经网络的参数，使得E尽快的缩小，这样我们就可以使神经网络尽快的达到我们期望的输出。首先考虑如何改变$w_{46}$才能使得误差减小。这里我求 $ \frac {\partial{E}} {\partial w_{46}}$ , 导数对应的方向函数上升下降的速度是最快的，我们使用偏导数找到最快的下降方向，按照这样的方式更新神经网络的权重参数和偏置参数。

$$
\frac {\partial E} {\partial w_{46}} = \frac {\partial E} {\partial O_6} * \frac {\partial O_6} {\partial I_6} * \frac {\partial I_6} {\partial w_{46}}
$$

上面应用了求导的链式法则，将对$w_{46}$的偏导数表示成了已知的数据。下面我们一步一步求出这个偏导数的表达式并得出参数的更新规则。

$$
E = \frac12 (target - O_6)^2
$$

$$
\frac {\partial E} {\partial O_6} = -\frac12*2(target-O_6) = O_6 - target = 0.473889 - 1 = -0.526111
$$

$$
O_6 = f(I_6) = \frac {1} {1+e^{-I_6}}
$$
$$
\frac {\partial O_6} {\partial I_6} {} = e^{-I_6}*(\frac {1} {1+e^{-I_6}})^{2} = O_6 * (1-O_6) = 0.473889 * (1-0.473889) = 0.249318
$$

$$
I_6 = O_4*w_{46}+O_5*w_{56}+b_6
$$

$$
\frac {\partial I_6} {\partial w_{46}} = O_4 = 0.331812
$$

综合上面的所有公式，可以推导出最终的参数更新公式是：
$$
\frac {\partial E} {\partial w_{46}} = (O_6 - target) * (O_6*(1-O_6))*O_4=(-0.526111)*0.249318*0.331812= -0.043523
$$

我们定义一个学习率$l$,表示参数更新的速度，这里设定为0.9；然后定义参数按照下面的方式更新：
$$
w^+_{46} = w_{46} - l * \frac {\partial E} {\partial w_{46}} = -0.3 - 0.9*(-0.043523) =  -0.260829
$$


下面总结一下上面提到的公式，需要注意的是，我这里举例的神经网络只有一个输出，所有偏导数和导数是一样的。但是通常的神经网络都有多个输出。在有多个输出的时候，我们定义E等于多个输出节点的E的和。这样，多个节点的输出就和单个节点的输出不太一样。我们定义输出节点的错误率为：
$$
E_i = O_i*(1-O_i)*(target-O_i)
$$

我们定义隐藏层的节点的错误率为：
$$
E_i = O_i*(1-O_i)*( \sum E_k*w_{jk})
$$

这样，每次参数的更新就可以写成下面的形式：
$$
\Delta w_{ij} = l * E_j * O_i
$$
偏置的更新定义如下：
$$
\Delta b_j = l * E_j
$$

不断的应用以上的公式更新权重参数和偏置，直到错误率到达一个很小的水平，停止训练。这个时候，神经网络对输入[1,0,1]的输出已经非常接近正确答案1了。

第一轮权重和偏置的更新是：
![](2017-04-19_213636.png)
